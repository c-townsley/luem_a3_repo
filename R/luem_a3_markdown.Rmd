---
title: "Estimating a Flood Inundation Probability Map"
subtitle: "CPLN 675 Assignment3"
author: "Riddhi Batra & Charlie Townsley"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
    code_folding: hide
    df_print: paged
    theme: flatly
    highlight: tango
---



# 1. Introduction

_The link to our video can be found at: ..._

The purpose of this project is to use existing flood inundation and environmental data from one city to build a predictive model that estimates flooding in a comparison city with similar characteristics. Our model is trained and validated on the city of Calgary, in Alberta, Canada. The city experienced record flooding in 2013 when heavy rainfall met with melting snowpack in the Rocky Mountains. Nearly 80,000 people were evacuated, and the flooding caused over $5 billion in property damage.^1^

Predictive flood modeling has important implications for the field of environmental planning. Floods cause most natural-disaster losses in the United States and are responsible for an annual average of nearly $8 billion in damage.^2^ Furthermore, as more people develop property in flood-prone areas, inundation induced losses will continue to increase. In the hands of government and community organizations, the results from predictive models are vital for helping cities plan, prepare, and respond to flooding.

With flood events increasing across the world, this project stresses the importance of predictive, comparision-based modeling for cities that face the potential of flooding, but have limited inundation history or access to real-time flood monitoring. Such an approach can help cities "on the edge" make comparisons to better understand their own flood exposure and consequently plan urban development, zoning and preservation policy, and disaster-mitigation with a better sense of preparedness. 


## Why Denver?

We chose Denver, Colorado as our comparison city for estimating flooding because of key similarities. We observed that:

* Both cities straddle rivers and are adjacent to the Rocky Mountains

* Both Calgary and Denver cover a large land area (319 mi^2^ vs. 155 mi^2^)

* They have similar levels of annual precipitation (16.7 in for Calgary and 8-15 in for Denver)

* They are both at high elevations (3,400 ft for Calgary, 5,400 ft for Denver)

* They both have flooded in the past


## Hypothesis

We began by thinking of which factors could influence and/or correlate with flood inundation. Based on [aerial images](https://www.servirglobal.net/Portals/0/Documents/Articles/ISERV%20Images/ISERV%20Calgary_flood%20V3.jpg) from the Calgary flood of 2013, we hypothesized that the probability of a grid cell in our model to flood is a function of proximity to rivers, elevation, land cover permeability, and where water has a natural tendency to flow and accumulation based on elevation and slope.


## Project Structure

This project follows the workflow below, which the following sections describe in detail:

1. Setup
     + Gather open data from Calgary and Denver
     + Process open data in ArcGIS to get our features of interest.


2. Create Fishnets and Wrangle data
     + Create a fishnet shapefile in R for each city
     + Use the Zonal Statistics as Table tool in ArcGIS to transform raster features to fishnet cell-level information
     + Join zonal statistics tables with fishnet shapefiles in R
     + Explore data in R


3. Run Logistic Regressions
     + Train model on Calgary Data
     + Refine model
     + Validate model
  

4. Interpretation

5. Map Predictions

6. Conclude with Closing Thoughts


## 1 Setup

Load libraries, set working directories, write map and plot themes, and call-in files. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

#knitr::opts_knit$set(root.dir = "C:/Users/ctown/OneDrive - PennO365/Classes/Classes_Sem4_2023Spring/CPLN 675_Land Modeling/Assignments/LUEM_Assignment3_FloodInundationProbability/Data") #Charlie's directory

knitr::opts_knit$set(root.dir = "C:/Users/rids2/PennO365/Townsley, Charlie - LUEM_Assignment3_FloodInundationProbability/Data") #Riddhi's Directory

rm(list=ls())

options(scipen = 999)

```

```{r libraries, echo=FALSE}
library(tidyverse)
library(sf)
library(raster)
library(data.table)
library(ggcorrplot)
library(caret)
library(pscl)
library(plotROC)
library(pROC)
library(kableExtra)
library(tigris)
library(viridis)
library(RColorBrewer)
```

```{r themes, results = "hide"}

mapTheme <- theme(plot.title =element_text(size=12),
                  plot.subtitle = element_text(size=8),
                  plot.caption = element_text(size = 6),
                  axis.line=element_blank(),
                  axis.text.x=element_blank(),
                  axis.text.y=element_blank(),
                  axis.ticks=element_blank(),
                  axis.title.x=element_blank(),
                  axis.title.y=element_blank(),
                  panel.background=element_blank(),
                  panel.border=element_blank(),
                  panel.grid.major=element_line(colour = 'transparent'),
                  panel.grid.minor=element_blank(),
                  legend.direction = "vertical", 
                  legend.position = "right",
                  plot.margin = margin(1, 1, 1, 1, 'cm'),
                  legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))

plotTheme <- theme(
  plot.title =element_text(size=12),
  plot.subtitle = element_text(size=8),
  plot.caption = element_text(size = 6),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title.y = element_text(size = 10),
  # Set the entire chart region to blank
  panel.background=element_blank(),
  plot.background=element_blank(),
  #panel.border=element_rect(colour="#F0F0F0"),
  # Format the grid
  panel.grid.major=element_line(colour="#D0D0D0",size=.75),
  axis.ticks=element_blank())

#color nerds

blues <- c("#CEEBF0", "#A2D3D8", "#73B8BF", "#51A6AE", "#468D94", "#34696E")

greens <- c("#D8E5CE", "#C1D4B5", "#A9BF99", "#92AF7E", "#81996F", "#668053")

yellows <- c("#F9E2B2", "#EDC876", "#EDBA46", "#EDB025", "#AC832D", "#6E5321")

greys <- c("#ECEBF1", "#D3D3D9", "#B0B0B3", "#9C9C9E", "#7F7F80", "#656666")

neutrals <- c("#FDF4E9", "#FFEFDE", "#F6D7B2", "#D6B28B", "#A58565")

```

```{r load_data, results = "hide"}
denver_boundary <-read_sf("Denver/Processed/LUEM_Asgn3_Denver/NewData/denver_bound/denver_bound.shp")

calgary_boundary <- read_sf("Calgary/Raw/CALGIS_CITYBOUND_LIMIT/CALGIS_CITYBOUND_LIMIT.shp")
```


# 2. Fishnets and Data Wrangling


## 2.1 Create Fishnets


Create a fishnet for Calgary to do processing in ArcGIS.

```{r create_fishnets_calgary, results = "hide"}

calgary_fishnet <- st_make_grid(calgary_boundary,
                        cellsize = 402.336,
                        square = FALSE) %>% 
  .[calgary_boundary] %>% 
  st_sf() %>% 
  mutate(uniqueID = rownames(.))

ggplot()+
  geom_sf(data = calgary_fishnet,
          fill = "lightgrey")+
  geom_sf(data = calgary_boundary, 
          color = "#6E5321", fill = "transparent") +
  mapTheme

#st_write(calgary_fishnet, "Calgary/Processed/calgary_fishnet/calgary_fishnet.shp", geometry = TRUE)

```


Create a fishnet for Denver to do processing in ArcGIS.

```{r create_fishnets_denver, results = "hide"}

denver_fishnet <- st_make_grid(denver_boundary,
                        cellsize = 1320,
                        square = FALSE) %>% 
  .[denver_boundary] %>% 
  st_sf() %>% 
  mutate(uniqueID = rownames(.))

ggplot()+
  geom_sf(data = denver_fishnet,
          color="darkgrey", fill = "lightgrey") +
  geom_sf(data = denver_boundary, 
          color = "#6E5321", fill = "transparent") +
  mapTheme

#st_write(denver_fishnet, "Denver/Processed/R exports/denver_fishnet/denver_fishnet.shp", geometry = TRUE)
```


## 2.2 Feature Engineering: Arc to R

After creating fishnets for Calgary and Denver in R, we exported them as shapefiles for processing in ArcGIS. We created tables of zonal statistics from relevant raster data, by fishnet cell, in ArcGIS. Then, we brought these tables back into R and joined each table with their respective city's fishnet.

```{r join engineered features to calg fishnet, results = "hide"}

#load in cleaned fishnet with partial cells removed
calgary_fishnet <-  read_sf("Calgary/Processed/calgary_fishnet_nozeros/calgary_fishnet_nozeros.shp")

#load in engineered features for calgary and process
calg_inund <- read_csv("Calgary/Processed/zonalstats_tables/calg_inundation_sum.csv") %>%
  rename(inund_sum = SUM) %>% 
  dplyr::select(uniqueID, inund_sum) %>% 
  mutate(inund_sum = ifelse(inund_sum >= 16, 1, 0)) #turn inundation sum values into binary (threshold = 16)

calg_pervious <- read.csv("Calgary/Processed/zonalstats_tables/calg_pervious_mean.csv") %>% 
  dplyr::select(uniqueID, MEAN) %>% 
  rename(pervious_mean = MEAN)

calg_elevation <- read.csv("Calgary/Processed/zonalstats_tables/calg_elevation_mean.csv") %>% 
    dplyr::select(uniqueID, MEAN) %>% 
  rename(elevation_mean = MEAN)

calg_flowac <- read.csv("Calgary/Processed/zonalstats_tables/calg_flowac_mean.csv") %>% 
    dplyr::select(uniqueID, MEAN) %>% 
  rename(flowac_mean = MEAN)

calg_streamdist <- read.csv("Calgary/Processed/zonalstats_tables/calg_dist2stream_min.csv") %>% 
    dplyr::select(uniqueID, MIN) %>% 
  rename(streamdist_min = MIN)

calg_dat <- calgary_fishnet %>%
  mutate(uniqueID = as.integer(uniqueID)) %>% 
  left_join(calg_inund, by = "uniqueID") %>% 
  left_join(calg_pervious, by = "uniqueID") %>%
  left_join(calg_elevation, by = "uniqueID") %>% 
  left_join(calg_flowac, by = "uniqueID") %>% 
  left_join(calg_streamdist, by = "uniqueID") %>% 
  mutate(flowac_mean_log = log(flowac_mean),
         streamdist_min_log = log(streamdist_min)) %>% 
  na.omit() %>% 
    dplyr::mutate(streamdist_min_log = if_else(streamdist_min_log <0, 0, streamdist_min_log),
         flowac_mean_log = if_else(flowac_mean_log <0, 0, flowac_mean_log)) %>% 
  na.omit()
  
```

```{r get_variables_denver, results = "hide"}

denv_pervious <- read.csv ("Denver/Processed/zonalstats_tables/denv_pervious_mean.csv") %>% 
  dplyr::select(uniqueID, MEAN) %>% 
  rename(pervious_mean = MEAN)

denv_elevation <- read.csv("Denver/Processed/zonalstats_tables/denver_elevation_mean.csv") %>% 
  dplyr::select(uniqueID, MEAN) %>% 
  rename(elevation_mean = MEAN)


denv_flowac <- read.csv("Denver/Processed/zonalstats_tables/denv_flowac_mean.csv") %>% 
    dplyr::select(uniqueID, MEAN) %>% 
  rename(flowac_mean = MEAN)


denv_streamdist <- read.csv("Denver/Processed/zonalstats_tables/denv_dist2stream_min.csv") %>% 
    dplyr::select(uniqueID, MIN) %>% 
  rename(streamdist_min = MIN)


denver_dat <- denver_fishnet %>%
  mutate(uniqueID = as.integer(uniqueID)) %>% 
  left_join(denv_pervious, by = 'uniqueID') %>%
  left_join(denv_elevation, by = 'uniqueID') %>% 
  left_join(denv_flowac, by = 'uniqueID') %>% 
  left_join(denv_streamdist, by = 'uniqueID') %>%
  mutate(flowac_mean = flowac_mean*0.3048,
         streamdist_min = streamdist_min*0.3048) %>% 
    mutate(flowac_mean_log = log(flowac_mean),
         streamdist_min_log = log(streamdist_min)) %>% 
  na.omit() %>% 
    dplyr::mutate(streamdist_min_log = if_else(streamdist_min_log <0, 0, streamdist_min_log),
         flowac_mean_log = if_else(flowac_mean_log <0, 0, flowac_mean_log))

```


### Calgary

Below are the features we created for Calgary:

* Flood inundation
* Pervious and impervious landcover
* Elevation
* Flow accumulation
* Distance to river


```{r calgary inundation map}
calg_dat <- calg_dat %>%
  st_transform(crs = 3776)

ggplot() +
  geom_sf(data=calg_dat, aes(fill=as.factor(inund_sum)), alpha = 0.8, color = NA) +
  scale_fill_manual(values = c("#CEEBF0", "#51A6AE"),
                    labels = c("Not Inundated", "Inundated"),
                    name = "Observed\nFlooding") +
  labs(title="Flood Inundation in Calgary",
       subtitle="Based on observed flooding in Calgary",
        caption = "Source: CPLN 675") +
  mapTheme
```

The Calgary flood inundation raster data we worked with contained four values: inundated, not inundated, rivers/water bodies, and clouds. We reclassified this layer so that everything inundated = 1, and everything else = 0. We then used Zonal Statistics as Table in Arc to calculate the sum of inundated raster cells for each fishnet cell. Finally, we changed these sum values into a binary in R. Every fishnet cell that had more than 16 inundated raster cells became equal to 1, and every other cell became equal to zero.

**Note: the Calgary inundation raster layer did not cover the entire city boundary. Therefore, the fishnet cells outside of the raster layer were removed from further processing.

```{r calgary stream dist map}
              
ggplot() +
  geom_sf(data=calg_dat, aes(fill=factor(ntile(streamdist_min, 4))), 
            colour=NA) +
  scale_fill_manual(values = yellows,
                    labels= as.character(round(quantile(calg_dat$streamdist_min,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T))),
                    name = "Distance\n(Quantile Breaks\nin Meters)") +
  labs(title="Distance from Rivers in Calgary",
      subtitle="Based on Calgary Hydrology Data",
      caption = "Source: data.calgary.ca") +
  mapTheme                    
                    
```

We used the distance accumulation tool to create a raster layered where each pixel was equal to its distance from a river. We then set each fishnet cell in the resulting table equal to the minimum distance to a river in that cell. We thought it best to over-sample cells near rivers rather than under-sample, because our model is based on riverine flooding.


```{r calgary fac map}

ggplot() +
  geom_sf(data=calg_dat, aes(fill=factor(ntile(flowac_mean,4))), 
            colour=NA) +
  scale_fill_manual(values = blues,
                    labels= as.character(round(quantile(calg_dat$flowac_mean,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T))),
                    name = "Mean Flow\nAccumulation\n(Quantile Breaks)") +
  labs(title="Precipitation Flow Accumulation in Calgary",
       subtitle="Based on Calgary Elevation Data",
        caption = "Source: 18M DEM, CPLN 675") +
  mapTheme

```

We used the DEM elevation layer to calculate flow accumulation in ArcGIS. Flow accumulation approximates where water that falls within city boundaries during a rain event would flow to by assigning each raster cell a value equal to the amount of cells that "flow" into it (based on elevation and flow direction). We then set each fishnet cell equal to the mean flow accumulation value it contained.


```{r calgary pervious surface map}

ggplot() +
  geom_sf(data=calg_dat, aes(fill=factor(ntile(pervious_mean,4))), 
            colour=NA) +
  scale_fill_manual(values = greens,
                    labels= as.character(round(quantile(calg_dat$pervious_mean,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T), 2)),
                    name = "Pervious\nSurface\n(Quantile\nBreaks)") +
  labs(title="Pervious Surface in Calgary",
       subtitle="Based on Calgary Land Cover Data",
        caption = "Source: data.calgary.ca") +
  mapTheme

```

We reclassified land cover raster data in Arc into two categories pervious surface = 1, and impervious surface = 0. We then set each fishnet cell in the resulting table equal to the mean of the pervious raster cells it contained. This was to account for a high amount of local variation in permeability.


```{r calgary elevation map}

ggplot() +
  geom_sf(data=calg_dat, aes(fill=factor(ntile(elevation_mean,4))), 
            colour=NA) +
  scale_fill_manual(values = neutrals,
                    labels= as.character(round(quantile(calg_dat$elevation_mean,
                                                         c(0.2,.4,.6,.8),
                                                         na.rm=T), 2)),
                    name = "Mean Elevation\n(Quantile Breaks\nCategorical)") +
  labs(title="Elevation in Calgary",
        caption = "Source: CPLN 675") +
    mapTheme


```

For elevation, we simply used the DEM for Calgary and set each fishnet cell equal to the median elevation in that cell.


### Denver

We performed the exact same operations for the Denver features, excluding inundation which we're setting out to predict.

```{r mapping denver }
denver_dat <- denver_dat %>%
  st_transform(crs = 2232)
```


```{r denver stream dist map}

ggplot() +
  geom_sf(data=denver_dat, aes(fill=factor(ntile(streamdist_min,4))), 
            colour=NA) +
  scale_fill_manual(values = yellows,
                    labels= as.character(round(quantile(denver_dat$streamdist_min,                                                    c(0.2,.4,.6,.8),
                                        na.rm=T), 2)),
     name = "Distance\n(Quantile Breaks\n in Metres)") +
  labs(title="Distance from Rivers in Denver",
       subtitle="Based on Denver Hydrology Data",
        caption = "Source: Denver Open Data Catalog") +
  mapTheme

```


```{r denver fac map}

ggplot() +
  geom_sf(data=denver_dat, aes(fill=factor(ntile(flowac_mean,4))), 
            colour=NA) +
  scale_fill_manual(values = blues,
                    labels= as.character(round(quantile(denver_dat$flowac_mean,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T))),
                    name = "Mean Flow\nAccumulation\n(Quantile Breaks)") +
  labs(title="Precipitation Flow Accumulation in Denver",
       subtitle="Based on Denver Elevation Data",
        caption = "Source: webgis.com") +
  mapTheme

```


```{r denver pervious surface map}

ggplot() +
  geom_sf(data=denver_dat, aes(fill=factor(ntile(pervious_mean,4))), 
            colour=NA) +
  scale_fill_manual(values = greens,
                    labels= as.character(round(quantile(denver_dat$pervious_mean,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T), 2)),
                    name = "Permeable\nSurface\n(Quantile\nBreaks)") +
  labs(title="Permeable Surface in Denver",
       subtitle="Based on Denver Land Cover Data",
        caption = "Source: Denver Regional Council of Governments Regional Data Catalog") +
  mapTheme

```
```{r denver elevation map}

ggplot() +
  geom_sf(data=denver_dat, aes(fill=factor(ntile(elevation_mean,4))), 
            colour=NA) +
  scale_fill_manual(values = neutrals,
                    labels= as.character(round(quantile(denver_dat$elevation_mean,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T))),
                    name = "Mean Elevation\n(Quantile Breaks\nCategorical)") +
  labs(title="Elevation in Denver",
        caption = "Source: webgis.com") +
  mapTheme

```


## 2.4 What is our Data Telling Us?


How do our variables correlate with inundated/not inundated values?

Note: we had to log adjust our Flow Accumulation and Distance to Stream variables to compensate the wide range of outliers in the data set. 

```{r wide_2_long}
calg_PlotVariables <- calg_dat %>% 
  as.data.frame() %>%
    dplyr::select(inund_sum, pervious_mean, elevation_mean, flowac_mean_log, streamdist_min_log) %>% 
    pivot_longer(cols = -inund_sum)
```

## Data Spread

The violin plots below shows how each variable is spread across 0/1 values of inundation. They visually indicate the relationship between the variables we assumed would cause / correlate to flooding, as a function of how we processed them with relation to Calgary's fishnet. For instance:

* Fishnet cells at lower elevations are more inundated than those at higher elevations.

* Fishnet cells at longer distances from a stream are not as inundated as those that are at shorter distances from a stream. 

By confirming the hypotheses we started with, this quick plot gives us confidence in our variable selection before we proceed with building regression models. 


``` {r data spread, results = "hide"}

#violin plots
#change code for for calgary_dat
##use boxplots with scatter points to visualize the spread of data?

ggplot(calg_PlotVariables) + 
     geom_violin(aes(x = as.factor(inund_sum), 
                  y = value, fill = as.factor(inund_sum))) + 
     facet_wrap(~name, scales = "free_y") +
     labs(x="Inundated", y="Value") + 
     scale_fill_manual(values = c("#CEEBF0", "#51A6AE"),
     labels = c("Not Inundated","Inundated"), name = "") +
     labs(x="Inundated", y="Value") + 
  plotTheme

##facet_wrap - one ggplot recipe for each variable
###use scales = free or free_y to plot values that are comparatively lower or higher
```


**How many fishnet cells in Calgary are inundated?**

A quick calculation reveals that 509 of 5373, or 9.5% of fishnet cells in Calgary are inundated.


``` {r fishnet cell calcs, results = "hide"}

calg_inund_fishnet <- calg_dat %>% 
  filter(inund_sum == 1)

no_fishnets <-(509/5373)*100


```


# 3. Logistic Regressions

## 3.1 Test, Train, Correlate, Regress: Model 1

Our first binomial regression model inputs the Calgary training set with the dependent variable set to "Inundation", and four independent variables, namely, Land Porosity (a binary derived from land cover), Elevation, Flow Accumulation, and Distance to a Stream.


```{r training_set, results = "hide"}

set.seed(3456)

trainIndex <- createDataPartition(calg_dat$elevation_mean, p = .70,
                                  list = FALSE,
                                  times = 1) 

inundTrain <- calg_dat[ trainIndex,] %>% 
  dplyr::select(-flowac_mean, -streamdist_min)

inundTest  <- calg_dat[-trainIndex,]%>% 
  dplyr::select(-flowac_mean, -streamdist_min)

##the sets are randomly generated
##p=0.70 indicates the 70/30 partition
```


## Correlation Tests


A quick correlation test reveals that there isn't too high a level of multi-collinearity between the variables we have selected for our regression. 

``` {r correlation matrix, results = "hide"}

corr <- calg_dat %>%
  as.data.frame() %>%
  dplyr::select(inund_sum, pervious_mean, elevation_mean, flowac_mean_log, streamdist_min_log) %>% 
  rename("Distance to Stream (log)" = streamdist_min_log,
         "Flow Accumulation (log)" = flowac_mean_log,
         "Elevation" = elevation_mean,
         "Land Porosity" = pervious_mean,
         "Inundation (observed)" = inund_sum)


calg_matrix = cor(corr)

ggcorrplot(calg_matrix, method="square", colors = c("#73BBBF", "#FDF4E9", "#92AF7E"),
           tl.cex=7)

```

A logistic regression on the first model displays the following results:

* The p-values indicate a high level of statistical significance for three out of four of our independent variables -- elevation, flow accumulation, and stream distance.

* The Aikake Information Criterion (AIC) is a useful number when comparing models on the basis of their performance. When selecting multiple regression models, a lower AIC indicates a better model performance.^3^ In this case, the AIC is **1259.8**.

* In our next step, we exponentiate the coefficients to interpret their relationship to the likelihood of flood inundation.


```{r firstModel, warning=FALSE}
inundModel <- glm(inund_sum ~ ., 
                    family="binomial"(link="logit"), data = inundTrain %>%
                                                            as.data.frame() %>%
                                                            dplyr::select(-geometry, -uniqueID))
summary(inundModel)
```

The exponentiated coefficients of the first model exhibit the following relationships between the dependent and independent variables:

* All else equal, a unit increase in Land Porosity _reduces_ the chances of inundation by 30.56% (this is awkward because Land Porosity is also a binary variable)
* All else equal, a unit increase in Elevation _reduces_ the chances of flood inundation by 47%
* All else equal, a unit change in Flow Accumulation _increases_ the odds of flood inundation by 13.9%
* All else equal, a unit increase in Distance to Stream _reduces_ the odds of flood inundation by 41.1%


``` {r firstModel coefficients, warning=FALSE, results="hide"}

## % change in Y for unit change in X = [(exponent (coefficient of X) - 1)] * 100
## - or + sign indicates associated increase or decrease

inundModel$coefficients

inundModel_vars <- c("Land Porosity", "Elevation", "Flow Accumulation (log)", "Distance to Stream (log)")
inundModel_coeffs <- c(((exp(-0.3648453 ) - 1) * 100), ((exp(-0.6357177) - 1) * 100), ((exp(0.1009900) - 1) * 100), ((exp( -0.5288784) - 1) * 100))

inundModel_coefficients <- data.frame(inundModel_vars, inundModel_coeffs)

inundModel_coefficients %>% 
  kbl(caption = "Exponentiated Coefficients: Logistic Regression Model 1") %>% 
   kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```


## 3.2 Regress: Model 2 & 3


Let's test out the results of some other models:


**Model 2: Does 'Land Porosity' matter?**

We eliminate Land Porosity, which did not exhibit statistical significance in our previous model.

* The resulting model shows that all independent variables are statistically significant.

* The AIC is very marginally lower than our first model, at **1259.2**.


```{r secondModel, warning=FALSE, results="hide"}

inundTrain_2 <- calg_dat[ trainIndex,] %>% 
  dplyr::select(-flowac_mean, -streamdist_min, -pervious_mean)

inundTest_2  <- calg_dat[-trainIndex,]%>% 
  dplyr::select(-flowac_mean, -streamdist_min, -pervious_mean)


inundModel_2 <- glm(inund_sum ~ ., 
                    family="binomial"(link="logit"), data = inundTrain_2 %>%
                                                            as.data.frame() %>%
                                                            dplyr::select(-geometry, -uniqueID))
summary(inundModel_2)

```


The exponentiated coefficients of the second model are not very different from the first model:

* All else equal, a unit change in Elevation _reduces_ the chances of flood inundation by 46.4%
* All else equal, a unit change in Flow Accumulation _increases_ the odds of flood inundation by 10.6%
* All else equal, a unit change in Distance to Stream _reduces_ the odds of flood inundation by 40.5%


``` {r secondModel coefficients, warning=FALSE, results="hide"}

## % change in Y for unit change in X = [(exponent (coefficient of X) - 1)] * 100
## - or + sign indicates associated increase or decrease

inundModel_2$coefficients

inundModel2_vars <- c("Elevation", "Flow Accumulation (log)", "Distance to Stream (log)")
inundModel2_coeffs <- c(((exp(-0.6243868) - 1) * 100), ((exp(0.1009589) - 1) * 100), ((exp(-0.5192783) - 1) * 100))

inundModel2_coefficients <- data.frame(inundModel2_vars, inundModel2_coeffs)

inundModel2_coefficients %>% 
  kbl(caption = "Exponentiated Coefficients: Logistic Regression Model 2") %>% 
   kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```

**Model 3: What if we don't log-adjust our variables?**

While carrying out a preliminary assessment of variable relationships, we log-adjusted Flow Accumulation and Distance to Stream to compress their range of values.

But what happens if we use the non-adjusted variables in a logit model?


* All variables except Land Porosity are statistically significant.

* The AIC is lower than the previous two models at **1185.2** (compared to 1253.8 and 1252.2 for models 1 and 2 respectively).

Does it mess with our coefficient interpretation?


```{r thirdModel, warning=FALSE, results="hide"}

inundTrain_3 <- calg_dat[ trainIndex,] %>% 
  dplyr::select(-flowac_mean_log, -streamdist_min_log)

inundTest_3  <- calg_dat[-trainIndex,]%>% 
  dplyr::select(-flowac_mean_log, -streamdist_min_log)


inundModel_3 <- glm(inund_sum ~ ., 
                    family="binomial"(link="logit"), data = inundTrain_3 %>%
                                                            as.data.frame() %>%
                                                            dplyr::select(-geometry, -uniqueID))
summary(inundModel_3)

```

The exponentiated coefficients of the third model are very different from the previous two models, in that the odds of inundation associated with Land Porosity, non-log-adjusted Flow Accumulation and Distance to Streams is a _lot_ smaller:

* All else equal, a unit change in Land Porosity _increases_ the chances of flood inundation by 4.9%
* All else equal, a unit change in Elevation _reduces_ the chances of flood inundation by 38.1%
* All else equal, a unit change in Flow Accumulation _increases_ the odds of flood inundation by 0.01%
* All else equal, a unit change in Distance to Stream _reduces_ the odds of flood inundation by 0.46%


This seems funky, given that a variation in distance from a stream should logically affect the odds of inundation a lot more than 0.46%.
Also, since this project is less about statistical significance and more about exploring variables, perhaps including Land Porosity would be fun.

We're glad (lol) we ran these options, but we're going to stick with our original model for the rest of the project ¯\_(ツ)_/¯

``` {r thirdModel coefficients, warning=FALSE, results="hide"}

## % change in Y for unit change in X = [(exponent (coefficient of X) - 1)] * 100
## - or + sign indicates associated increase or decrease

inundModel_3$coefficients

inundModel3_vars <- c("Land Porosity", "Elevation", "Flow Accumulation", "Distance to Stream")
inundModel3_coeffs <- c(((exp(0.0479009901) - 1) * 100), ((exp(-0.4802761173) - 1) * 100), ((exp(0.0001509916) - 1) * 100), ((exp(-0.0046012831) - 1) * 100))

inundModel3_coefficients <- data.frame(inundModel3_vars, inundModel3_coeffs)

inundModel3_coefficients %>% 
  kbl(caption = "Exponentiated Coefficients: Logistic Regression Model 2") %>% 
   kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```


## 3.3 Model Validation


**So, Is My House Going to Flood?**

Well, it depends on where you live.


The plots below illustrate a distribution of predicted probabilities, based on the training and test sets. 

* The histogram illustrates the predicted probability of fishnet cells in Calgary being inundated, based on the conditions set by our first model. As the frequency distribution shows, the dataset exhibits a lower overall probability of inundation -- there are over 1200 fishnet cells with a 0-0.1 probability of inundation, whereas cells with a higher probability of inundation are much fewer.

* The second plot is a measure of how well our data is predicting probabilities for "inundation" (1s) vs "no inundation" (0s), where the vertical line represents a 0.5 probability of inundation. In this case the values reflecting not inundated areas are clustered closer to zero, indicating a lower probability for inundation overall.

But, are these predictions accurate?



```{r predict_first}
classProbs <- predict(inundModel, inundTest, type="response")


hist(classProbs)
  

##histogram is for the whole dataset
##represents the probability that a cell will be inundated (x-axis), vs number of cells with that probability (y-axis)
```

```{r plot_preds}
testProbs <- data.frame(obs = as.numeric(inundTest$inund_sum),
                        pred = classProbs)

ggplot(testProbs, aes(x = pred, fill=as.factor(obs))) + 
  geom_density() +
  facet_grid(obs ~ .) + 
  xlab("Probability") + 
  ylab("Frequency") +
  geom_vline(xintercept = .5) +
  scale_fill_manual(values = c("#CEEBF0", "#51A6AE"),
                      labels = c("Not Inundated","Inundated")) +
  labs(title = "Number of Fishnet Cells Associated with Flooding in Calgary")+
  plotTheme


```


# 4. Confusion, Indeed

## 4.1 Confusion Metrics

To test the model's prediction accuracy, we created a confusion matrix from which levels of error can be extrapolated.
If we assume a probability cutoff threshold of 50%, our confusion matrix gives us an accuracy of **0.909**, with the following sensitivity and specificity results:



**Sensitivity & Specificity Analysis**: Predicted vs. Reference Values, 50% Cutoff


* It has predicted 0 as 0, i.e. a _True Negative_, 1389 times
* It has predicted 0 as 1, i.e. a _False Negative_, 91 times
* It has predicted 1 as 0, i.e. a _False Positive_, 55 times
* It has predicted 1 as 1, i.e. a _True Positive_, 75 times


Overall, the model's true positive rate, i.e., the proportion of 1s predicted as 1s indicates its _sensitivity_, which in this case is almost twice as much as the number of 1s falsely predicted as 0s. The model's true negative rate, i.e., the proportion of 0s accurately predicted as 0s is about 15 times greater than the number of 0s falsely predicted as 1s, indicating the model's _specificity_.

It has an error rate of about 10% as derived from the accuracy value (Error = Accuracy - 1), which is demonstrated by the fact that the model's 146 erroneous predictions are only a small fraction of its 1464 accurate predictions.

```{r confusion_matrix, warning=FALSE}

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")
```

We experimented with the threshold cutoff to check how the model would respond, and found that:

The accuracy of a model with with a **75%** threshold cutoff is a little higher than the original model with a 50% cutoff, at **0.917**. However, it contains slightly different levels of False Positive and False Negative predictions.

Similarly, the accuracy of a model with a **25%** cutoff is only marginally higher than the original model at **0.903**, but it also exhibits a lower specificity.


```{r confusion_matrix_75_25, message = FALSE, warning = FALSE}

testProbs$predClass75  = ifelse(testProbs$pred > .75 ,1,0)

caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass75), 
                       positive = "1")

testProbs$predClass25  = ifelse(testProbs$pred > .25 ,1,0)

caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass25), 
                       positive = "1")

```

## 4.2 ROC Curve


**Run, These Are No Geese!!**

The Receiver Operating Characteristic (ROC) curve for our original model with a 50% cutoff plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) for the probability of flood inundation in Calgary.

It shows the trade-off between Sensitivity and Specificity in our test set against a random classifier where True Positive = False Positive (the light grey diagonal). In this case, the variation of the ROC curve away from the random classifier line, toward the top-right corner indicates a high rate of model accuracy.

This is further confirmed by calculating the area under the curve as **0.939**, which confirms that the model is able to predict flood inundation in Calgary with a 94% accuracy. 


But how well would it predict on other datasets?


```{r roc_curve, message = FALSE, warning = FALSE}

ggplot(testProbs, aes(d = obs, m = pred)) + 
  geom_roc(n.cuts = 50, labels = FALSE) + 
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') 

```

```{r auc, warning = FALSE}
auc(testProbs$obs, testProbs$pred)
```



## 4.3 Cross Validation


**It's Accurate but is it Generalizable?**

The following section checks the accuracy of our predictions across 100 randomly generated test sets, to gauge its applicability in predicting Denver's chances of flood inundation.

On average, the prediction accuracy across all 100 folds is **92%**.


```{r k_fold, warning = FALSE, message = FALSE}

ctrl <- trainControl(method = "cv", 
                     number = 100, 
                     savePredictions = TRUE)

inundFit <- train(as.factor(inund_sum) ~ .,
               data = calg_dat %>% 
                 as.data.frame() %>%
                 dplyr::select(inund_sum, pervious_mean, elevation_mean, flowac_mean_log, streamdist_min_log), 
               method="glm", family="binomial",
               trControl = ctrl)

inundFit

#inundFit is our model trained to predict using the binomial logistic regression, or glm, method. 
```

Subsequently plotting a histogram of the accuracy for each fold allows us to trace its generalizability.
The plot points out that a large number of folds (i.e., values from the "Resample" column) are clustered at high accuracy values. 
This indicates a high level of generalizability -- a measure of the model's capacity to be applied to predict other sample sets -- in our case, flood inundation in Denver.

It gives the model confidence in moving to the final stage of the project -- applying Calgary's flood inundation predictions to the city of Denver. 

```{r cv_hist, warning = FALSE, message = FALSE}

ggplot(as.data.frame(inundFit$resample), aes(Accuracy)) + 
  geom_histogram() +
  scale_x_continuous(limits = c(0, 1)) +
  labs(x="Accuracy",
       y="Count")+
  plotTheme
```

# 5. Map Predictions

**The Moment We've Been Waiting For**

We ran a lot of tests on our model, and are probably ready to use it for a few predictions, now. 

## 5.1 Predictions for Calgary

```{r predict_whole, warning = FALSE, message= FALSE}

calg_dat_log <- calg_dat %>% 
  dplyr::select(uniqueID, inund_sum, pervious_mean, elevation_mean, flowac_mean_log, streamdist_min_log, geometry)


allPredictions <- 
  predict(inundFit, calg_dat, type="prob")[,2]
  
calg_pred <- 
  cbind(calg_dat_log,allPredictions) %>%
  mutate(allPredictions = round(allPredictions * 100)) 
```


```{r predicted_map1, warning = FALSE, message = FALSE}
 ggplot() + 
    geom_sf(data=calg_pred, aes(fill=factor(ntile(allPredictions,4))), 
            colour=NA) +
    scale_fill_manual(values = blues,
                      labels=as.character(quantile(calg_pred$allPredictions,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T)),
                      name="Predicted\nProbabilities(%)\n(Grouped in\nQuantile\nBreaks)") +
  mapTheme +
  labs(title="Predicted Probability of Flood Inundation in Calgary",
       subtitle = "Based on a Logistic Regression Model")
```

Let’s map it again with the already calculated inundation types overlaid.

```{r predicted_map2, warning = FALSE, message = FALSE}
 ggplot() + 
  geom_sf(data=calg_pred, aes(fill=factor(ntile(allPredictions,4))), colour=NA) +
  scale_fill_manual(values = blues,
                    labels=as.character(quantile(calg_pred$allPredictions,
                                                 c(.2,.4,.6,.8),
                                                 na.rm=T)),
                    name="Predicted\nProbabilities(%)\n(Grouped in\nQuintile\nBreaks)") +
  geom_sf(data=calg_pred  %>% 
               filter(inund_sum == 1), 
               fill="#EDBA46", alpha=0.9, colour=NA) +
    geom_sf(data=calg_pred %>% 
              filter(inund_sum == 0), 
            fill="#F9E2B2", alpha=0.35,colour=NA) +  
  mapTheme +
  labs(title="Observed and Predicted Flood Inundation Areas",
       subtitle="Yellow marks areas with observed 'inundation', \nall other taken as 'not inundated' for the purpose of binary regression modeling")
```

Looks like our predicted and observed inundation values are very close!

How are the Sensitivity and Specificity values laid out on the map of Calgary? 


```{r error_map, warning = FALSE, message= FALSE}
calg_pred %>%
  mutate(confResult=case_when(allPredictions < 50 & inund_sum==0 ~ "True Negative",
                              allPredictions >= 50 & inund_sum==1 ~ "True Positive",
                              allPredictions < 50 & inund_sum==1 ~ "False Negative",
                              allPredictions >= 50 & inund_sum==0 ~ "False Positive")) %>%
  ggplot()+
  geom_sf(aes(fill = confResult), color = "transparent")+
  scale_fill_manual(values = c("#B0B0B3","#A2D3D8","#FFEFDE","#81996F"),
                    name="Outcomes")+
  labs(title="Confusion Metrics") +
  mapTheme

```

In spite of the confusion matrix indicating low levels of false negatives proportionate to the data we processed for Calgary, the map helps spatially visualize how the model's errors are spread out, and what might be contributing to them:

* In pre-processing our data, we reclassified the Land Cover rasters for both cities to a 0/1 binary to indicate Land Porosity, where we kept urbanized and rocky land as "non-pervious", whereas all natural vegetation and marshes were "pervious". This assumption could have reduced the model's accuracy by reducing continuous run-off potential numbers to binary, thus reducing nuances in the relationship between Land Porosity and Inundation that the model could capture. 


## 5.2 Predictions for Denver

```{r predict_whole_denver, warning = FALSE, message= FALSE}

denver_dat_log <- denver_dat %>% 
  dplyr::select(uniqueID, pervious_mean, elevation_mean, flowac_mean_log, streamdist_min_log, geometry)

allPredictions_denver <- 
  predict(inundFit, denver_dat_log, type="prob")[,2]
  
denver_pred <- 
  cbind(denver_dat_log, allPredictions_denver) %>% 
  mutate(allPredictions_denver = round(allPredictions_denver * 100)) 



```

       
```{r predicted_map_denver, warning = FALSE, message = FALSE}


 ggplot() + 
     geom_sf(data=denver_pred, aes(fill=factor(ntile(allPredictions_denver,4))), 
            colour=NA) +
    scale_fill_manual(values = blues,
                      labels=as.character(quantile(denver_pred$allPredictions_denver,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T)),
                      name="Predicted\nProbabilities(%)\n(Grouped in\nQuantile\nBreaks)") +
  mapTheme +
  labs(title="Predicted Probability of Flood Inundation in Denver",
        subtitle="Based on a Logistic Regression Model trained on data from Calgary\n")

  
```

Does this line up with Denver's hydrological features?

``` {r pred map 2 denver, warning = FALSE, message = FALSE}

denver_hydro <- read_sf("Denver/Raw/streams/streams.shp")

  ggplot() + 
     geom_sf(data=denver_pred, aes(fill=factor(ntile(allPredictions_denver,4))), 
            colour=NA) +
    scale_fill_manual(values = blues,
                      labels=as.character(quantile(denver_pred$allPredictions_denver,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T)),
                      name="Predicted\nProbabilities(%)\n(Quantile\nBreaks)") +
   geom_sf(data=denver_hydro, color="#EDB025", size=35, linejoin="round", lineend="round") +
  labs(title="Predicted Probability of Flood Inundation in Denver",
        subtitle="Based on a Logistic Regression Model trained on data from Calgary\nYellow Lines Mark Location of Existing Rivers & Streams",
        caption = "Source: blah blah blah") +
   mapTheme


```

Yes, it actually lines up very well!

Phew, can't believe that worked...



# 6. Conclusion


**Prediction Defense**

Our model might not be the Oracle of Delphi, but given the large number of underlying assumptions, it works well on a number of fronts:

* It has a high level of accuracy and generalizability, as shown by its accuracy of 92%, even on average across a 100-fold cross-validation test, and the high number of true negatives and positives compared to false negatives and false positives.

* The predicted and observed values for Calgary line up well on the map, with variations observed in how each variable has been visualized - the former on an increasing categorical scale, and the later as a 0/1 binary.

* It accurately placed predicted inundation in Denver along the location of Denver's existing streams, thus confirming its high level of accuracy and generalizability. 

This type of machine learning process differs from a site suitability study in that it builds an algorithm based on probability and predictions, rather than a visual assessment of existing site conditions. This can be helpful in situations like flood inundation mapping, when present-day data is required to communicate a range of odds for an event that might occur in the future. In addition to incorporating more data than a site suitability study, it can be a useful tool for policy-makers to drive disaster-response or land preservation programs. 

It is also quicker than a more detailed, hydraulic model, to generate spatial scenarios of relative risk, that, for a city planning department can help prioritize how more detailed analytical resources are deployed.


**Next Steps**

If we were doing this for **$$**, we could send to Denver municipal government with policy recommendations for zoning and flood response in low-lying areas close to streams in Denver, subject to further hydraulic modeling.


And if we were to run through this project again (we won't, but _if_ we were), we might do a few things differently:

* Find a way to integrate land porosity through its run-off potential values, rather than use a simplified binary classification based on urbanized/non-urbanized land.

* Incorporate some more variables from the stream network analysis, such as flow direction, that might help build the robustness of our model. 

* Run through the similarities and differences between the datastes for our two cities from the get go. We struggled for a while with a strange prediction of the Calgary dataset on Denver, and eventually learnt that:
** By not ensuring both datasets had variables in the same unit (either feet or metres), we were confusing the model
** By entering Elevation data as absolute numbers instead of in discrete bins, we were training the model on Calgary's relatively high range of elevations (900-1500 metres), which then read Denver's elevations (all below 900 metres), as completely inundated. 

* Get more familiar with how our datasets speak to each other, and to the model, from the get go, because that was a steep learning curve for us.


For now, we're going to get some sleep.


xx
Charlie + Riddhi



## 6.1 Endnotes

1. City of Calgary. “Flooding in Calgary - Flood of 2013.” www.calgary.ca. Accessed March 27, 2023. https://www.calgary.ca/content/www/en/home/water/flooding/history-calgary.html.

2. “Flood Inundation Mapping (FIM) Program.” n.d. U.S. Geological Survey. Accessed February 27, 2023. https://www.usgs.gov/mission-areas/water-resources/science/flood-inundation-mapping-fim-program.

3. Hastie, T., Tibshirani, R., Friedman, J. 2009. The Elements of Statistical Learning: Data Mining, Interference, and Prediction. Second Ed. Available at: https://link.springer.com/book/10.1007/978-0-387-84858-7

