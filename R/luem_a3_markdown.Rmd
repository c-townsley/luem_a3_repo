---
title: "CPLN 675_Assignment3"
author: "Riddhi Batra & Charlie Townsley"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
    code_folding: hide
    df_print: paged
    theme: flatly
    highlight: tango
---

# 1. Introduction

The purpose of this project is to...

...The Planning motivation for this algorithm and how we would deploy such an algorithm.


**Hypothesis**

We began by thinking of the factors that would influence and/or correlate with flood inundation....


**Why Denver?**

The choice of Denver as a comparison city against which to test the model we built on Calgary was based on a check of their geographical and demographic similarities. We noticed that:

* Population and size..

* Climate..

* Hydro features..

* While both metropolitan areas are at an elevation range of ___ to ___, both Calgary and Denver themselves are at a lower elevation than the mountains that surround them, so the variation in elevation within the cities' boundaries is not as dramatic as if we considered the DEM outside the cities' municipal boundaries. 


**Project Structure**

This project follows a machine learning workflow that....



## 1.1 Setup


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#knitr::opts_knit$set(root.dir = "C:/Users/ctown/OneDrive - PennO365/Classes/Classes_Sem4_2023Spring/CPLN 675_Land Modeling/Assignments/LUEM_Assignment3_FloodInundationProbability/Data") #Charlie's directory

knitr::opts_knit$set(root.dir = "C:/Users/rids2/PennO365/Townsley, Charlie - LUEM_Assignment3_FloodInundationProbability/Data") #Riddhi's Directory

rm(list=ls())

options(scipen = 999)

```

```{r libraries, echo=FALSE}
library(tidyverse)
library(sf)
library(raster)
library(data.table)
library(ggcorrplot)
library(caret)
library(pscl)
library(plotROC)
library(pROC)
library(kableExtra)
library(tigris)
library(viridis)
library(RColorBrewer)
```

```{r themes, warning = FALSE, message = FALSE, results = "hide"}

mapTheme <- theme(plot.title =element_text(size=12),
                  plot.subtitle = element_text(size=8),
                  plot.caption = element_text(size = 6),
                  axis.line=element_blank(),
                  axis.text.x=element_blank(),
                  axis.text.y=element_blank(),
                  axis.ticks=element_blank(),
                  axis.title.x=element_blank(),
                  axis.title.y=element_blank(),
                  panel.background=element_blank(),
                  panel.border=element_blank(),
                  panel.grid.major=element_line(colour = 'transparent'),
                  panel.grid.minor=element_blank(),
                  legend.direction = "vertical", 
                  legend.position = "right",
                  plot.margin = margin(1, 1, 1, 1, 'cm'),
                  legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))

plotTheme <- theme(
  plot.title =element_text(size=12),
  plot.subtitle = element_text(size=8),
  plot.caption = element_text(size = 6),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title.y = element_text(size = 10),
  # Set the entire chart region to blank
  panel.background=element_blank(),
  plot.background=element_blank(),
  #panel.border=element_rect(colour="#F0F0F0"),
  # Format the grid
  panel.grid.major=element_line(colour="#D0D0D0",size=.75),
  axis.ticks=element_blank())

#color nerds

blues <- c("#CEEBF0", "#A2D3D8", "#73B8BF", "#51A6AE", "#468D94", "#34696E")

greens <- c("#D8E5CE", "#C1D4B5", "#A9BF99", "#92AF7E", "#81996F", "#668053")

yellows <- c("#F9E2B2", "#EDC876", "#EDBA46", "#EDB025", "#AC832D", "#6E5321")

greys <- c("#ECEBF1", "#D3D3D9", "#B0B0B3", "#9C9C9E", "#7F7F80", "#656666")

neutrals <- c("#FDF4E9", "#FFEFDE", "#F6D7B2", "#D6B28B", "#A58565")

```

## 1.2 Load Data

```{r load_data, warning = FALSE, message = FALSE, results = "hide"}
denver_boundary <-read_sf("Denver/Processed/LUEM_Asgn3_Denver/NewData/denver_bound/denver_bound.shp")

calgary_boundary <- read_sf("Calgary/Raw/CALGIS_CITYBOUND_LIMIT/CALGIS_CITYBOUND_LIMIT.shp")
```


# 2. Fishnets and Data Wrangling


## 2.1 Create Fishnets


Create a fishnet for Calgary to do processing in ArcGIS.

```{r create_fishnets_calgary, warning = FALSE, message = FALSE, results = "hide"}

calgary_fishnet <- st_make_grid(calgary_boundary,
                        cellsize = 402.336,
                        square = FALSE) %>% 
  .[calgary_boundary] %>% 
  st_sf() %>% 
  mutate(uniqueID = rownames(.))

ggplot()+
  geom_sf(data = calgary_fishnet,
          fill = "lightgrey")+
  geom_sf(data = calgary_boundary, 
          color = "#6E5321", fill = "transparent") +
  mapTheme

#st_write(calgary_fishnet, "Calgary/Processed/calgary_fishnet/calgary_fishnet.shp", geometry = TRUE)

```


Create a fishnet for Denver to do processing in ArcGIS.

```{r create_fishnets_denver, warning = FALSE, message = FALSE, results = "hide"}

denver_fishnet <- st_make_grid(denver_boundary,
                        cellsize = 1320,
                        square = FALSE) %>% 
  .[denver_boundary] %>% 
  st_sf() %>% 
  mutate(uniqueID = rownames(.))

ggplot()+
  geom_sf(data = denver_fishnet,
          color="darkgrey", fill = "lightgrey") +
  geom_sf(data = denver_boundary, 
          color = "#6E5321", fill = "transparent") +
  mapTheme

#st_write(denver_fishnet, "Denver/Processed/R exports/denver_fishnet/denver_fishnet.shp", geometry = TRUE)
```

## 2.2 Feature Engineering: Arc to R

Processed raster features for Calgary are exported from ArcGIS as tables containing zonal statistics by fishnet cell. These tables are then re-imported to R and joined with their respective fishnets. The list of features in question is:
* Flood inundation
* Pervious and impervious landcover
* Elevation
* Flow accumulation
* Distance to river

```{r join engineered features to calg fishnet, warning = FALSE, message = FALSE, results = "hide"}
#load in cleaned fishnet with partial cells removed
calgary_fishnet <-  read_sf("Calgary/Processed/calgary_fishnet_nozeros/calgary_fishnet_nozeros.shp")

#load in engineered features for calgary and process
calg_inund <- read_csv("Calgary/Processed/zonalstats_tables/calg_inundation_sum.csv") %>%
  rename(inund_sum = SUM) %>% 
  dplyr::select(uniqueID, inund_sum) %>% 
  mutate(inund_sum = ifelse(inund_sum >= 16, 1, 0)) #turn inundation sum values into binary (threshold = 16)

calg_pervious <- read.csv("Calgary/Processed/zonalstats_tables/calg_pervious_mean.csv") %>% 
  dplyr::select(uniqueID, MEAN) %>% 
  rename(pervious_mean = MEAN)

calg_elevation <- read.csv("Calgary/Processed/zonalstats_tables/calg_elevation_med.csv") %>% 
    dplyr::select(uniqueID, MEDIAN) %>% 
  rename(elevation_median = MEDIAN)


calg_flowac <- read.csv("Calgary/Processed/zonalstats_tables/calg_flowac_mean.csv") %>% 
    dplyr::select(uniqueID, MEAN) %>% 
  rename(flowac_mean = MEAN)


calg_streamdist <- read.csv("Calgary/Processed/zonalstats_tables/calg_dist2stream_min.csv") %>% 
    dplyr::select(uniqueID, MIN) %>% 
  rename(streamdist_min = MIN)

calg_dat <- calgary_fishnet %>%
  mutate(uniqueID = as.integer(uniqueID)) %>% 
  left_join(calg_inund, by = "uniqueID") %>% 
  left_join(calg_pervious, by = "uniqueID") %>%
  left_join(calg_elevation, by = "uniqueID") %>% 
  left_join(calg_flowac, by = "uniqueID") %>% 
  left_join(calg_streamdist, by = "uniqueID") %>% 
  mutate(flowac_mean_log = log(flowac_mean),
         streamdist_min_log = log(streamdist_min)) %>% 
  na.omit() %>% 
    dplyr::mutate(streamdist_min_log = if_else(streamdist_min_log <0, 0, streamdist_min_log),
         flowac_mean_log = if_else(flowac_mean_log <0, 0, flowac_mean_log)) %>% 
  na.omit()
  
```


The same process is followed for Denver with the same features, excluding inundation.

```{r get_variables_denver, warning = FALSE, message = FALSE, results = "hide"}

denv_pervious <- read.csv ("Denver/Processed/zonalstats_tables/denv_pervious_mean.csv") %>% 
  dplyr::select(uniqueID, MEAN) %>% 
  rename(pervious_mean = MEAN)

denv_elevation <- read.csv("Denver/Processed/zonalstats_tables/denv_elevation_med.csv") %>% 
    dplyr::select(uniqueID, MEDIAN) %>% 
  rename(elevation_median = MEDIAN)


denv_flowac <- read.csv("Denver/Processed/zonalstats_tables/denv_flowac_mean.csv") %>% 
    dplyr::select(uniqueID, MEAN) %>% 
  rename(flowac_mean = MEAN)


denv_streamdist <- read.csv("Denver/Processed/zonalstats_tables/denv_dist2stream_min.csv") %>% 
    dplyr::select(uniqueID, MIN) %>% 
  rename(streamdist_min = MIN)


denver_dat <- denver_fishnet %>%
  mutate(uniqueID = as.integer(uniqueID)) %>% 
  left_join(denv_pervious, by = 'uniqueID') %>%
  left_join(denv_elevation, by = 'uniqueID') %>% 
  left_join(denv_flowac, by = 'uniqueID') %>% 
  left_join(denv_streamdist, by = 'uniqueID') %>% 
    mutate(flowac_mean_log = log(flowac_mean),
         streamdist_min_log = log(streamdist_min)) %>% 
  na.omit() %>% 
    dplyr::mutate(streamdist_min_log = if_else(streamdist_min_log <0, 0, streamdist_min_log),
         flowac_mean_log = if_else(flowac_mean_log <0, 0, flowac_mean_log))
  

```


## 2.3 Mapping Features

Mapping the fishnet of calgary with these features reveals...

```{r mapping calgary inundation}
calg_dat <- calg_dat %>%
  st_transform(crs = 3776)

ggplot() +
  geom_sf(data = calgary_boundary, aes(fill="#FFEFDE", color = "#656666"))+
  geom_sf(data=calg_dat, aes(fill=as.factor(inund_sum)), alpha = 0.8, color = "#D3D3D9") +
  scale_fill_manual(values = c("#FFEFDE", "#34696E"),
                    labels = c("Not Inundated", "Inundated"), name = "") +
  labs(title="Flood Inundation in Calgary") +
  mapTheme

```


```{r mapping denver }
denver_dat <- denver_dat %>%
  st_transform(crs = 2232)

```



## 2.4 What is our Data Telling Us?


How do our variables correlate with inundated/not inundated values?

Note: we had to log adjust our Flow Accumulation and Distance to Stream variables to compensate the wide range of outliers in the data set. 

```{r wide_2_long}
calg_PlotVariables <- calg_dat %>% 
  as.data.frame() %>%
    dplyr::select(inund_sum, pervious_mean, elevation_median, flowac_mean_log, streamdist_min_log) %>% 
    pivot_longer(cols = -inund_sum)

```

The violin plots below shows how each variable is spread across 0/1 values of inundation. They visually indicate the relationship between the variables we assumed would cause / correlate to flooding, as a function of how we processed them with relation to Calgary's fishnet. For instance:

* Fishnet cells at lower elevations are more inundated than those at higher elevations.

* Fishnet cells at longer distances from a stream are not as inundated as those that are at shorter distances from a stream. 

By confirming the hypotheses we started with, this quick plot gives us confidence in our variable selection before we proceed with building regression models. 


``` {r data spread, warning = FALSE, message = FALSE, results = "hide"}

#violin plots
#change code for for calgary_dat
##use boxplots with scatter points to visualize the spread of data?

ggplot(calg_PlotVariables) + 
     geom_violin(aes(x = as.factor(inund_sum), 
                  y = value, fill = as.factor(inund_sum))) + 
     facet_wrap(~name, scales = "free_y") +
     labs(x="Inundated", y="Value") + 
     scale_fill_manual(values = c("#CEEBF0", "#51A6AE"),
     labels = c("Not Inundated","Inundated"), name = "") +
     labs(x="Inundated", y="Value") + 
  plotTheme

##facet_wrap - one ggplot recipe for each variable
###use scales = free or free_y to plot values that are comparatively lower or higher
```


**How many fishnet cells in Calgary are inundated?**

A quick calculation reveals that 509 of 5373, or 9.5% of fishnet cells in Calgary are inundated.


``` {r fishnet cell calcs, warning = FALSE, message = FALSE, results = "hide"}

calg_inund_fishnet <- calg_dat %>% 
  filter(inund_sum == 1)

no_fishnets <-(509/5373)*100


```


# 3. Logistic Regressions

## 3.1 Test, Train, Correlate, Regress: Model 1

Our first binomial regression model inputs the Calgary training set with the dependent variable set to "Inundation", and four independent variables, namely, Land Porosity (a binary derived from land cover), Elevation, Flow Accumulation, and Distance to a Stream.


```{r training_set, warning = FALSE, message = FALSE, results = "hide"}

set.seed(3456)

trainIndex <- createDataPartition(calg_dat$elevation_median, p = .70,
                                  list = FALSE,
                                  times = 1) 

inundTrain <- calg_dat[ trainIndex,] %>% 
  dplyr::select(-flowac_mean, -streamdist_min)

inundTest  <- calg_dat[-trainIndex,]%>% 
  dplyr::select(-flowac_mean, -streamdist_min)

##the sets are randomly generated
##p=0.70 indicates the 70/30 partition
```

A quick correlation test reveals that there isn't too high a level of multicollinearity between the variables we have selected for our regression. 

``` {r correlation matrix, warning = FALSE, message = FALSE, results = "hide"}

corr <- calg_dat %>%
  as.data.frame() %>%
  dplyr::select(inund_sum, pervious_mean, elevation_median, flowac_mean_log, streamdist_min_log) %>% 
  rename("Distance to Stream (log)" = streamdist_min_log,
         "Flow Accumulation (log)" = flowac_mean_log,
         "Elevation" = elevation_median,
         "Land Porosity" = pervious_mean,
         "Inundation (observed)" = inund_sum)


calg_matrix = cor(corr)

ggcorrplot(calg_matrix, method="square", colors = c("#73BBBF", "#FDF4E9", "#92AF7E"),
           tl.cex=7)

```

A logistic regression on the first model displays the following results:

* The p-values indicate a high level of statistical significance for three out of four of our independent variables -- elevation, flow accumulation, and stream distance.

* The Aikake Information Criterion (AIC) is a useful number when comparing models on the basis of their performance. When selecting multiple regression models, a lower AIC indicates a better model performance^1^. In this case, the AIC is **1253.8**.

* In our next step, we exponentiate the coefficients to interpret their relationship to the likelihood of flood inundation.


```{r firstModel, warning = FALSE, message = FALSE}
inundModel <- glm(inund_sum ~ ., 
                    family="binomial"(link="logit"), data = inundTrain %>%
                                                            as.data.frame() %>%
                                                            dplyr::select(-geometry, -uniqueID))
summary(inundModel)
```

The exponentiated coefficients of the first model exhibit the following relationships between the dependent and independent variables:

* All else equal, a unit change in Land Porosity _reduces_ the chances of inundation by 16.35% (this is awkward because Land Porosity is also a binary variable)
* All else equal, a unit change in Elevation _reduces_ the chances of flood inundation by 2.7%
* All else equal, a unit change in Flow Accumulation _increases_ the odds of flood inundation by 13.9%
* All else equal, a unit change in Distance to Stream _reduces_ the odds of flood inundation by 38.7%


``` {r firstModel coefficients, warning = FALSE, message = FALSE}

## % change in Y for unit change in X = [(exponent (coefficient of X) - 1)] * 100
## - or + sign indicates associated increase or decrease

inundModel$coefficients

inundModel_vars <- c("Land Porosity", "Elevation", "Flow Accumulation (log)", "Distance to Stream (log)")
inundModel_coeffs <- c(((exp(-0.17860495) - 1) * 100), ((exp(-0.02753326) - 1) * 100), ((exp(0.13087261) - 1) * 100), ((exp(-0.49010423 ) - 1) * 100))

inundModel_coefficients <- data.frame(inundModel_vars, inundModel_coeffs)

inundModel_coefficients %>% 
  kbl(caption = "Exponentiated Coefficients: Logistic Regression Model 1") %>% 
   kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```


## 3.2 Regress: Model 2 & 3


Let's test out the results of some other models:


**Model 2: Does 'Land Porosity' matter?**

We eliminate Land Porosity, which did not exhibit statistical significance in our previous model.

* The resulting model shows that all independent variables are statistically significant.

* The AIC is marginally lower than our first model, at **1252.2**.


```{r secondModel, warning = FALSE, message = FALSE}

inundTrain_2 <- calg_dat[ trainIndex,] %>% 
  dplyr::select(-flowac_mean, -streamdist_min, -pervious_mean)

inundTest_2  <- calg_dat[-trainIndex,]%>% 
  dplyr::select(-flowac_mean, -streamdist_min, -pervious_mean)


inundModel_2 <- glm(inund_sum ~ ., 
                    family="binomial"(link="logit"), data = inundTrain_2 %>%
                                                            as.data.frame() %>%
                                                            dplyr::select(-geometry, -uniqueID))
summary(inundModel_2)

```


The exponentiated coefficients of the second model are not very different from the first model:

* All else equal, a unit change in Elevation _reduces_ the chances of flood inundation by 2.6%
* All else equal, a unit change in Flow Accumulation _increases_ the odds of flood inundation by 13.9%
* All else equal, a unit change in Distance to Stream _reduces_ the odds of flood inundation by 38.4%


``` {r secondModel coefficients, warning = FALSE, message = FALSE}

## % change in Y for unit change in X = [(exponent (coefficient of X) - 1)] * 100
## - or + sign indicates associated increase or decrease

inundModel_2$coefficients

inundModel2_vars <- c("Elevation", "Flow Accumulation (log)", "Distance to Stream (log)")
inundModel2_coeffs <- c(((exp(-0.02727086) - 1) * 100), ((exp(0.13073865) - 1) * 100), ((exp(-0.48586608) - 1) * 100))

inundModel2_coefficients <- data.frame(inundModel2_vars, inundModel2_coeffs)

inundModel2_coefficients %>% 
  kbl(caption = "Exponentiated Coefficients: Logistic Regression Model 2") %>% 
   kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```

**Model 3: What if we don't log-adjust our variables?**

While carrying out a preliminary assessment of variable relationships, we log-adjusted Flow Accumulation and Distance to Stream to compress their range of values.

But what happens if we use the non-adjusted variables in a logit model?


* All variables except Land Porosity are statistically significant.

* The AIC is lower than the previous two models at **1185.2** (compared to 1253.8 and 1252.2 for models 1 and 2 respectively).

Does it mess with our coefficient interpretation?


```{r thirdModel, warning = FALSE, message = FALSE}

inundTrain_3 <- calg_dat[ trainIndex,] %>% 
  dplyr::select(-flowac_mean_log, -streamdist_min_log)

inundTest_3  <- calg_dat[-trainIndex,]%>% 
  dplyr::select(-flowac_mean_log, -streamdist_min_log)


inundModel_3 <- glm(inund_sum ~ ., 
                    family="binomial"(link="logit"), data = inundTrain_3 %>%
                                                            as.data.frame() %>%
                                                            dplyr::select(-geometry, -uniqueID))
summary(inundModel_3)

```

The exponentiated coefficients of the third model are different from the previous two models, in that the odds of inundation associated with non-log-adjusted Flow Accumulation and Distance to Streams is a _lot_ smaller:

* All else equal, a unit change in Land Porosity _increases_ the chances of flood inundation by 22%
* All else equal, a unit change in Elevation _reduces_ the chances of flood inundation by 2.1%
* All else equal, a unit change in Flow Accumulation _increases_ the odds of flood inundation by 0.01%
* All else equal, a unit change in Distance to Stream _reduces_ the odds of flood inundation by 0.42%


This seems funky, given that a variation in distance from a stream should logically affect the odds of inundation a lot more than 0.42%.
Also, since this project is less about statistical significance and more about exploring variables, perhaps including Land Porosity would be fun.

We're glad (lol) we ran these options, but we're going to stick with our original model for the rest of the project ¯\_(ツ)_/¯

``` {r thirdModel coefficients, warning = FALSE, message = FALSE}

## % change in Y for unit change in X = [(exponent (coefficient of X) - 1)] * 100
## - or + sign indicates associated increase or decrease

inundModel_3$coefficients

inundModel3_vars <- c("Land Porosity", "Elevation", "Flow Accumulation", "Distance to Stream")
inundModel3_coeffs <- c(((exp(0.1989610743) - 1) * 100), ((exp(-0.0215381673) - 1) * 100), ((exp(0.0001608351) - 1) * 100), ((exp(-0.0042834696) - 1) * 100))

inundModel3_coefficients <- data.frame(inundModel3_vars, inundModel3_coeffs)

inundModel3_coefficients %>% 
  kbl(caption = "Exponentiated Coefficients: Logistic Regression Model 2") %>% 
   kable_styling(bootstrap_options = "striped", full_width = F, position = "left")

```


## 3.3 Model Validation

**So, Is My House Going to Flood?**

Well, it depends on where you live.

The plots below illustrate a distribution of predicted probabilities, based on the training and test sets. 

* The histogram illustrates the predicted probability of fishnet cells in Calgary being inundated, based on the conditions set by our first model. As the frequency distribution shows, the dataset exhibits a lower overall probability of inundation -- there are over 1200 fishnet cells with a 0-0.1 probability of inundation, whereas cells with a higher probability of inundation are much fewer.

* The second plot is a measure of how well our data is predicting probabilities for "inundation" (1s) vs "no inundation" (0s), where the vertical line represents a 0.5 probability of inundation. In this case the values reflecting not inundated areas are clustered closer to zero, indicating a lower probability for inundation overall.


```{r predict_first}
classProbs <- predict(inundModel, inundTest, type="response")

hist(classProbs)

##histogram is for the whole dataset
##represents the probability that a cell will be inundated (x-axis), vs number of cells with that probability (y-axis)
```

```{r plot_preds}
testProbs <- data.frame(obs = as.numeric(inundTest$inund_sum),
                        pred = classProbs)

ggplot(testProbs, aes(x = pred, fill=as.factor(obs))) + 
  geom_density() +
  facet_grid(obs ~ .) + 
  xlab("Probability") + 
  ylab("Frequency") +
  geom_vline(xintercept = .5) +
  scale_fill_manual(values = c("#CEEBF0", "#51A6AE"),
                      labels = c("Not Inundated","Inundated"),
                      name = "")+
  plotTheme


```


# 4. Confusion, Indeed

## 4.1 Confusion Metrics

To test the model's prediction accuracy, we created a confusion matrix from which levels of error can be extrapolated.
If we assume a probability cutoff threshold of 50%, our confusion matrix gives us an accuracy of **0.923**, with the following sensitivity and specificity results:


**Sensitivity & Specificity Analysis**
_Predicted vs. Reference Values, 50% Cutoff_

-------------------
|   | 0    | 1    |
-------------------
| 0 | 1414 | 46   |
| 1 | 77   | 74   |
-------------------


* It has predicted 0 as 0, i.e. a _True Negative_, 1414 times
* It has predicted 0 as 1, i.e. a _False Negative_, 77 times
* It has predicted 1 as 0, i.e. a _False Positive_, 46 times
* It has predicted 1 as 1, i.e. a _True Positive_, 74 times

Overall, the model's true positive rate, i.e., the proportion of 1s predicted as 1s indicates its _sensitivity_, which in this case is almost twice as much as the number of 1s falsely predicted as 0s. The model's true negative rate, i.e., the proportion of 0s accurately predicted as 0s is about 18 times greater than the number of 0s falsely predicted as 1s, indicating the model's _specificity_.

It has an error rate of about 8% as derived from the accuracy value (Error = Accuracy - 1), which is demonstrated by the fact that the model's 123 erroneous predictions (77+46) are only a small fraction of its 1488 accurate predictions (1414+74).

```{r confusion_matrix, message = FALSE, warning = FALSE}

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")
```

We experimented with the threshold cutoff to check how the model would respond, and found that:

The accuracy of a model with with a 75% threshold cutoff remains the same as the original model with a 50% cutoff.
However, it contains slightly different levels of False Positive and False Negative predictions:


**Sensitivity & Specificity Analysis**
_Predicted vs. Reference Values, 75% Cutoff_

-------------------
|   | 0    | 1    |
-------------------
| 0 | 1454 | 118  |
| 1 | 6    | 33   |
-------------------


Similarly, the accuracy of a model with a 25% cutoff is only marginally lower than the original model at 0.918, but it also exhibits a lower specificity:


**Sensitivity & Specificity Analysis**
_Predicted vs. Reference Values, 25% Cutoff_

-------------------
|   | 0    | 1    |
-------------------
| 0 | 1383 | 55   |
| 1 | 77   | 96   |
-------------------


```{r confusion_matrix_75_25, message = FALSE, warning = FALSE}

testProbs$predClass75  = ifelse(testProbs$pred > .75 ,1,0)

caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass75), 
                       positive = "1")

testProbs$predClass25  = ifelse(testProbs$pred > .25 ,1,0)

caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass25), 
                       positive = "1")

```

## 4.2 ROC Curve

**Run, These Are No Geese!!**

The Receiver Operating Characteristic (ROC) curve for our original model with a 50% cutoff plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) for the probability of flood inundation in Calgary.

It shows the trade-off between Sensitivity and Specificity in our test set against a random classifier where True Positive = False Positive (the light grey diagonal). In this case, the variation of the ROC curve away from the random classifier line, toward the top-right corner indicates a high rate of model accuracy.

This is further confirmed by calculating the area under the curve as **0.947**, which confirms that the model is able to predict flood inundation with a 95% accuracy. 


```{r roc_curve, message = FALSE, warning = FALSE}

ggplot(testProbs, aes(d = obs, m = pred)) + 
  geom_roc(n.cuts = 50, labels = FALSE) + 
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') 

```

```{r auc, warning = FALSE}
auc(testProbs$obs, testProbs$pred)
```


## 4.3 Cross Validation

**It's Accurate but is it Generalizable?**

The following section checks the accuracy of our predictions across 100 randomly generated test sets, to gauge its applicability in predicting Denver's chances of flood inundation.

On average, the prediction accuracy across all 100 folds is 92%.


```{r k_fold, warning = FALSE, message = FALSE}

ctrl <- trainControl(method = "cv", 
                     number = 100, 
                     savePredictions = TRUE)

inundFit <- train(as.factor(inund_sum) ~ .,
               data = calg_dat %>% 
                 as.data.frame() %>%
                 dplyr::select(inund_sum, pervious_mean, elevation_median, flowac_mean_log, streamdist_min_log), 
               method="glm", family="binomial",
               trControl = ctrl)

inundFit

#inundFit is our model trained to predict using the binomial logistic regression, or glm, method. 
```
Subsequently plotting a histogram of the accuracy for each fold allows us to trace its generalizability.
The plot points out that a large number of folds (i.e., values from the "Resample" column) are clustered at high accuracy values. 
This indicates a high level of generalizability -- a measure of the model's capacity to be applied to predict other sample sets -- in our case, flood inundation in Denver.

It gives the model confidence in moving to the final stage of the project -- applying Calgary's flood inundation predictions to the city of Denver. 

```{r cv_hist, warning = FALSE, message = FALSE}

ggplot(as.data.frame(inundFit$resample), aes(Accuracy)) + 
  geom_histogram() +
  scale_x_continuous(limits = c(0, 1)) +
  labs(x="Accuracy",
       y="Count")+
  plotTheme
```

# 5. Map Predictions

**The Moment We've Been Waiting For**

We ran a lot of tests on our model, and are probably ready to use it for a few predictions, now. 

## 5.1 Predictions for Calgary

```{r predict_whole, warning = FALSE, message= FALSE}

calg_dat_log <- calg_dat %>% 
  dplyr::select(uniqueID, inund_sum, pervious_mean, elevation_median, flowac_mean_log, streamdist_min_log, geometry) 

allPredictions <- 
  predict(inundFit, calg_dat_log, type="prob")[,2]
  
calg_pred <- 
  cbind(calg_dat_log,allPredictions) %>%
  mutate(allPredictions = round(allPredictions * 100)) 
```


```{r predicted_map1, warning = FALSE, message = FALSE}
 ggplot() + 
    geom_sf(data=calg_pred, aes(fill=factor(ntile(allPredictions,4))), 
            colour=NA) +
    scale_fill_manual(values = blues,
                      labels=as.character(quantile(calg_pred$allPredictions,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T)),
                      name="Predicted\nProbabilities(%)\n(Quantile\nBreaks)") +
  mapTheme +
  labs(title="Predicted Probability of Flood Inundation in Calgary")
```

Let’s map it again with the already calculated inundation types overlaid.

```{r predicted_map2, warning = FALSE, message = FALSE}
 ggplot() + 
  geom_sf(data=calg_pred, aes(fill=factor(ntile(allPredictions,4))), colour=NA) +
  scale_fill_manual(values = blues,
                    labels=as.character(quantile(calg_pred$allPredictions,
                                                 c(.2,.4,.6,.8),
                                                 na.rm=T)),
                    name="Predicted\nProbabilities(%)\n(Quintile\nBreaks)") +
  geom_sf(data=calg_pred  %>% 
               filter(inund_sum == 1), 
               fill="#EDBA46", alpha=0.9, colour=NA) +
    geom_sf(data=calg_pred %>% 
              filter(inund_sum == 0), 
            fill="#F9E2B2", alpha=0.35,colour=NA) +  
  mapTheme +
  labs(title="Observed and Predicted Flood Inundation Areas",
       subtitle="Yellow marks areas with observed 'inundation', \nall other taken as 'not inundated' for the purpose of binary regression modeling")
```

Looks like our predicted and observed values are close!

How are the Sensitivity and Specificity values laid out on the map of Calgary? 


```{r error_map, warning = FALSE, message= FALSE}
calg_pred %>%
  mutate(confResult=case_when(allPredictions < 50 & inund_sum==0 ~ "True Negative",
                              allPredictions >= 50 & inund_sum==1 ~ "True Positive",
                              allPredictions < 50 & inund_sum==1 ~ "False Negative",
                              allPredictions >= 50 & inund_sum==0 ~ "False Positive")) %>%
  ggplot()+
  geom_sf(aes(fill = confResult), color = "transparent")+
  scale_fill_manual(values = c("#B0B0B3","#A2D3D8","#FFEFDE","#81996F"),
                    name="Outcomes")+
  labs(title="Confusion Metrics") +
  mapTheme

```

In spite of the confusion matrix indicating low levels of false negatives proportionate to the data we processed for Calgary, the map helps spatially visualize how the model's errors are spread out, and what might be contributing to them:

* Flow Accumulation ...?

* Land Cover ... ? 


## 5.2 Predictions for Denver

```{r predict_whole_denver, warning = FALSE, message= FALSE}

denver_dat_log <- denver_dat %>% 
  dplyr::select(uniqueID, pervious_mean, elevation_median, flowac_mean_log, streamdist_min_log, geometry)

allPredictions_denver <- 
  predict(cvFit, denver_dat_log, type="prob")

str(allPredictions_denver)

allPredictions_denver <- allPredictions_denver %>% dplyr::select(c(2))
  
denver_pred <- 
  cbind(denver_dat_log,allPredictions_denver) %>%
  rename(allPredictions=X1)%>%
  mutate(allPredictions = allPredictions* 10000000) %>% 
    mutate(allPredictions = if_else(allPredictions >= 0.5, 1, 0))


```

       
```{r predicted_map_denver, warning = FALSE, message = FALSE}


 ggplot() + 
    geom_sf(data=denver_pred, aes(fill=factor(allPredictions)), 
            colour=NA) +
    scale_fill_manual(values = c("#CEEBF0", "#51A6AE"),
                      labels=c("0", "1"),
                      name="Predicted\nProbabilities\n(Binary)") +
  mapTheme +
  labs(title="Predicted Probability of Flood Inundation in Denver",
        subtitle="Based on a Logistic Regression Model trained on data from Calgary\n",
        caption = "Source: blah blah blah")
  
```
Does this line up with Denver's hydrological features?

``` {r pred map 2 denver, warning = FALSE, message = FALSE}

denver_hydro <- read_sf("Denver/Raw/streams/streams.shp")

 ggplot() + 
       geom_sf(data=denver_pred, aes(fill=factor(allPredictions)), 
            colour=NA) +
    scale_fill_manual(values = c("#CEEBF0", "#51A6AE"),
                      labels=c("0", "1"),
                      name="Predicted\nProbabilities\n(Binary)") +
   geom_sf(data=denver_hydro, color="#EDB025", size=35, linejoin="round", lineend="round") +
  labs(title="Predicted Probability of Flood Inundation in Denver",
        subtitle="Based on a Logistic Regression Model trained on data from Calgary\nYellow Lines Mark Location of Existing Rivers & Streams",
        caption = "Source: blah blah blah") +
   mapTheme


```
Yes, it actually lines up very well!

Phew, can't believe that worked...


# 6. Conclusion

Our model might not be the Oracle of Delphi, but given the large number of underlying assumptions, it works well on a number of fronts:

* It has a high level of accuracy and generalizability, as shown by its accuracy of 92%, even on average across a 100-fold cross-validation test, and the high number of true negatives and positives compared to false negatives and false positives.

* The predicted and observed values for Calgary line up well on the map, with variations observed in how each variable has been visualized - the former on an increasing categorical scale, and the later as a 0/1 binary.

* ....

This type of machine learning process differs from a site suitability study in that it builds an algorithm based on probability and predictions, rather than a visual assessment of existing site conditions. This can be helpful in situations like flood inundation mapping, when present-day data is required to communicate a range of odds for an event that might occur in the future. In addition to incorporating more data than a site suitability study, it can be a useful tool for policy-makers to drive disaster-response or land preservation programs. 

If we were to run through this project again (we won't, but _if_ we were), we might do a few things differently:

* Find a way to integrate land porosity through its run-off potential values, rather than use a simplified binary classification based on urbanized/non-urbanized land.

* Incorporate some more variables from the stream network analysis, such as flow direction, that might help build the robustness of our model. 

* ....

For now, we're going to get some sleep.


xx
Charlie + Riddhi


## 6.1 Endnotes

1. Hastie, T., Tibshirani, R., Friedman, J. 2009. The Elements of Statistical Learning: Data Mining, Interference, and Prediction. Second Ed. Available at: https://link.springer.com/book/10.1007/978-0-387-84858-7



``` {r code test}

ggplot() + 
    geom_sf(data=denver_pred, aes(fill=factor(ntile(allPredictions,4))), 
            colour=NA) +
    scale_fill_manual(values = blues,
                      labels=as.character(quantile(denver_pred$allPredictions,
                                                 c(0.2,.4,.6,.8),
                                                 na.rm=T)),
                      name="Predicted\nProbabilities(%)\n(Quintile\nBreaks)") +
   geom_sf(data=denver_hydro, color="#EDB025", size=20) +
  labs(title="Predicted Probability of Flood Inundation in Denver",
        subtitle="Based on a Logistic Regression Model trained on data from Calgary\nYellow Lines Mark Location of Existing Rivers & Streams",
        caption = "Source: blah blah blah") +
   mapTheme

```

